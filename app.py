import streamlit as st
import pandas as pd
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import uuid
import tempfile
import wave
import numpy as np
from faster_whisper import WhisperModel
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, RTCConfiguration, WebRtcMode
import av

# -----------------------------
# ì´ˆê¸° ì„¸íŒ…
# -----------------------------
st.set_page_config(page_title="Vector Search Chatbot with Voice", layout="wide")
st.title("ğŸ’¬ ìŒì„± ì…ë ¥ ì •ì±… ì„œë¹„ìŠ¤ ê²€ìƒ‰ ì±—ë´‡")

# OpenAI API Key ì…ë ¥
openai_api_key = st.sidebar.text_input("ğŸ”‘ OpenAI API Key", type="password")

if 'openai_api_key' not in st.session_state:
    st.session_state.openai_api_key = None

if openai_api_key:
    st.session_state.openai_api_key = openai_api_key

# Qdrant ì„¤ì •
st.sidebar.header("Qdrant ì„¤ì •")
qdrant_host = st.sidebar.text_input("Qdrant Host", "localhost")
qdrant_port = st.sidebar.number_input("Qdrant Port", value=6333)
collection_name = st.sidebar.text_input("Collection Name", "demo_collection")

# ëª¨ë¸ ë¡œë”©
@st.cache_resource
def load_model():
    return SentenceTransformer("jhgan/ko-sroberta-multitask")

embedder = load_model()

# Whisper ëª¨ë¸ ë¡œë”© (CPU ì „ìš©, int8 ìµœì í™”)
@st.cache_resource
def load_stt_model():
    return WhisperModel("small", device="cpu", compute_type="int8")

stt_model = load_stt_model()

# Qdrant client ì—°ê²°
qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)

# OpenAI í´ë¼ì´ì–¸íŠ¸
def get_openai_client():
    if st.session_state.openai_api_key:
        return OpenAI(api_key=st.session_state.openai_api_key)
    return None


# -----------------------------
# CSV ì—…ë¡œë“œ
# -----------------------------
st.header("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    st.dataframe(df.head())

    text_columns = df.columns.tolist()
    main_col = st.selectbox("ì„œë¹„ìŠ¤ëª…ì„ ë‚˜íƒ€ë‚´ëŠ” ì»¬ëŸ¼ ì„ íƒ", text_columns)
    support_cols = st.multiselect("ì¶”ê°€ ì •ë³´ë¥¼ ë‹´ì€ ì»¬ëŸ¼ ì„ íƒ", text_columns, default=[c for c in text_columns if c != main_col])

    if st.button("Qdrantì— ì—…ë¡œë“œ"):
        qdrant_client.recreate_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=embedder.get_sentence_embedding_dimension(),
                                        distance=Distance.COSINE)
        )
        points = []
        for _, row in df.iterrows():
            concat_text = str(row[main_col])
            for c in support_cols:
                concat_text += " " + str(row[c])

            vector = embedder.encode(concat_text).tolist()
            payload = {col: str(row[col]) for col in df.columns}
            points.append(PointStruct(id=str(uuid.uuid4()), vector=vector, payload=payload))

        qdrant_client.upsert(collection_name=collection_name, points=points)
        st.success(f"âœ… {len(points)}ê°œ í•­ëª©ì„ Qdrantì— ì—…ë¡œë“œ ì™„ë£Œ!")


# -----------------------------
# ìŒì„± ì…ë ¥ â†’ í…ìŠ¤íŠ¸ ë³€í™˜
# -----------------------------
st.header("ğŸ™ ìŒì„± ì…ë ¥")

class AudioProcessor(AudioProcessorBase):
    def __init__(self) -> None:
        self.audio_frames = []

    def recv_audio(self, frame: av.AudioFrame) -> av.AudioFrame:
        audio = frame.to_ndarray()
        self.audio_frames.append(audio)
        return frame

rtc_configuration = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

ctx = webrtc_streamer(
    key="speech",
    mode=WebRtcMode.SENDRECV,
    media_stream_constraints={"audio": True, "video": False},
)

query_text = st.text_input("âœï¸ ì§ì ‘ í…ìŠ¤íŠ¸ ì…ë ¥", placeholder="ì˜ˆ: ì²­ë…„ ì£¼ê±° ì§€ì› ë°›ì„ ìˆ˜ ìˆëŠ” ì œë„ ì•Œë ¤ì¤˜")

if ctx.audio_processor:
    if st.button("ğŸ™ ìŒì„± ë³€í™˜ â†’ í…ìŠ¤íŠ¸ ì…ë ¥"):
        pcm_data = np.concatenate(ctx.audio_processor.audio_frames, axis=1).T
        tmp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        with wave.open(tmp_wav.name, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(pcm_data.astype(np.int16).tobytes())

        segments, _ = stt_model.transcribe(tmp_wav.name, beam_size=5)
        recognized_text = " ".join([seg.text for seg in segments])

        st.session_state["query_text"] = recognized_text
        st.success("ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: " + recognized_text)

# -----------------------------
# ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ + GPT ìš”ì•½
# -----------------------------
st.header("ğŸ” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ & ìš”ì•½ ë‹µë³€")

query = st.session_state.get("query_text", query_text)
top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", 1, 10, 5)

if st.button("ê²€ìƒ‰ ì‹¤í–‰"):
    if not query:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        q_emb = embedder.encode(query).tolist()
        results = qdrant_client.search(collection_name=collection_name, query_vector=q_emb, limit=top_k)

        if not results:
            st.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            context_texts = []
            for r in results:
                p = r.payload
                main_text = p.get(main_col, "")
                extras = [f"{col}: {p.get(col, '')}" for col in support_cols]
                context_texts.append(f"- {main_text}\n  " + "\n  ".join(extras))

            context = "\n\n".join(context_texts)
            prompt = f"""
ë‹¹ì‹ ì€ ì •ë¶€ ë³µì§€/ì •ì±… ì„œë¹„ìŠ¤ ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ í›„ë³´ ì •ì±…ë“¤ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ê²Œ ì ì ˆíˆ ìš”ì•½í•˜ì—¬ ë‹µë³€í•´ ì£¼ì„¸ìš”. 

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ í›„ë³´ ì •ì±…:
{context}

ìš”ì•½ ê·œì¹™:
- ì„œë¹„ìŠ¤ëª…ì€ ì œëª©ì²˜ëŸ¼ ê°•ì¡°
- ì§€ì›ë‚´ìš©, ì‹ ì²­ê¸°í•œ ë“±ì€ í•µì‹¬ë§Œ ì •ë¦¬
- ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ì„œë¹„ìŠ¤ë§Œ ì„ íƒí•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…
- ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë‹µë³€

ìµœì¢… ë‹µë³€:
"""

            openai_client = get_openai_client()
            
            if openai_client:
                try:
                    response = openai_client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "system", "content": "ë„ˆëŠ” ì •ë¶€ ì§€ì› ì •ì±…ì„ ìš”ì•½í•´ì„œ ì•Œë ¤ì£¼ëŠ” ì¹œì ˆí•œ ìƒë‹´ì›ì´ë‹¤."},
                            {"role": "user", "content": prompt}
                        ]
                    )
                    answer = response.choices[0].message.content
                    st.markdown("### ğŸ¤– ì±—ë´‡ ë‹µë³€")
                    st.write(answer)
                except Exception as e:
                    st.error(f"OpenAI API ì˜¤ë¥˜: {e}")
            else:
                st.error("âš ï¸ OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
